from __future__ import annotations

import csv
import logging
import re
from collections import defaultdict
from pathlib import Path
from typing import Any

import requests
import yaml

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO
)

BASE = Path(__file__).resolve().parent.parent
CONTRIBS_PATH = BASE / "contribs.yaml"

RAW_GITHUB_BASE = "https://raw.githubusercontent.com/ome/ngff/refs/heads/main/"

REQUEST_TIMEOUT = 30

KNOWN_FIELDS = {"role", "name", "github_handle", "institution", "date", "status"}
DEFAULT_HEADERS = ["role", "name", "github_handle", "institution", "date", "status"]


#####        This parsing block was partially generated by Codex 5.3       #####
# In particular, the details of markdown parsing were NOT revised individually #
# only the output was checked for reasonableness.                              #
# The main loop logic, structure and source choices were implemented by hand.  #
################################################################################


def main() -> None:

    # Get list of files in the repository for checking if comments are missing
    # (e.g. if we have a commenter role but no comment files, that may indicate an issue)
    url = "https://api.github.com/repos/ome/ngff/git/trees/main?recursive=1"
    response = requests.get(url, timeout=REQUEST_TIMEOUT)
    response.raise_for_status()
    repo_files = {
        item["path"]
        for item in response.json().get("tree", [])
        if item["type"] == "blob"
    }

    data = load_contribs_data()

    # Get all the rfc/{number}/index.md files from repo_files
    rfc_numbers = []
    for file_path in repo_files:
        match = re.match(r"^rfc/(\d+)/index\.md$", file_path)
        if match:
            rfc_numbers.append(int(match.group(1)))

    parsed_rfcs: dict[str, dict[str, list[str]]] = {}

    for rfc_number in rfc_numbers:
        url = RAW_GITHUB_BASE + f"rfc/{rfc_number}/index.md"

        # Count comments for this RFC by looking for files like rfc/{number}/comments/*.md
        comment_files = [
            file_path
            for file_path in repo_files
            if re.match(rf"^rfc/{rfc_number}/comments/[0-9]+/.*\.md$", file_path)
        ]

        markdown_content = fetch_text(url)
        records = parse_list_tables(markdown_content)
        records.extend(parse_markdown_tables(markdown_content))
        records = deduplicate_records(records)

        if not records:
            logging.info(
                "RFC %s: no contributor table found (likely still in preparation)",
                rfc_number,
            )
            continue

        role_names = records_to_role_names(records)
        if not role_names:
            logging.warning("RFC %s: parsed rows but extracted no names", rfc_number)
            continue

        parsed_rfcs[f"rfc{rfc_number}"] = role_names
        summary = ", ".join(
            f"{role}={len(names)}" for role, names in sorted(role_names.items())
        )
        logging.info("RFC %s: %s", rfc_number, summary)

        logging.info("RFC %s: found %d comment files", rfc_number, len(comment_files))

        # Warn if there are more comments than commenters
        if len(comment_files) > len(role_names.get("commenter", [])):
            logging.warning(
                "RFC %s: found %d comment files but %d commenters",
                rfc_number,
                len(comment_files),
                len(role_names.get("commenter", [])),
            )

    data["rfcs"] = dict(sorted(parsed_rfcs.items(), key=lambda item: int(item[0][3:])))

    CONTRIBS_PATH.write_text(yaml.dump(data, sort_keys=False, allow_unicode=True))
    logging.info("Updated %s with %d RFC entries", CONTRIBS_PATH, len(parsed_rfcs))


def fetch_text(url: str) -> str:
    response = requests.get(url, timeout=REQUEST_TIMEOUT)
    response.raise_for_status()
    return response.text


def normalize_whitespace(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()


def strip_markdown(text: str) -> str:
    value = text.strip()
    if not value:
        return ""
    # Keep only link text: [text](url) -> text
    value = re.sub(r"\[([^\]]+)\]\(([^)]+)\)", r"\1", value)
    value = value.replace("**", "").replace("__", "")
    value = value.replace("`", "")
    value = normalize_whitespace(value)
    if value == "-":
        return ""
    return value


def normalize_header(header: str) -> str:
    normalized = strip_markdown(header).lower()
    normalized = re.sub(r"[^a-z0-9 ]+", " ", normalized)
    normalized = normalize_whitespace(normalized)
    if normalized == "role":
        return "role"
    if normalized == "name":
        return "name"
    if "github" in normalized:
        return "github_handle"
    if "institution" in normalized:
        return "institution"
    if normalized == "date":
        return "date"
    if normalized == "status":
        return "status"
    return ""


def looks_like_header_row(headers: list[str]) -> bool:
    recognized = {header for header in headers if header in KNOWN_FIELDS}
    return "name" in recognized and len(recognized) >= 2


def normalize_row_length(cells: list[str], headers: list[str]) -> list[str]:
    if len(cells) < len(headers):
        return cells + [""] * (len(headers) - len(cells))
    if len(cells) == len(headers):
        return cells
    # If we have extra split columns, keep the tail in the last cell.
    return cells[: len(headers) - 1] + [" ".join(cells[len(headers) - 1 :])]


def cells_to_record(cells: list[str], headers: list[str]) -> dict[str, str]:
    normalized_cells = normalize_row_length(cells, headers)
    record: dict[str, str] = {}
    for header, cell in zip(headers, normalized_cells):
        if header in KNOWN_FIELDS:
            record[header] = strip_markdown(cell)
    return record


def parse_list_table_block(block_content: str) -> list[dict[str, str]]:
    rows: list[list[str]] = []
    current_row: list[str] | None = None

    for raw_line in block_content.splitlines():
        line = raw_line.rstrip()
        if not line.strip():
            continue
        if line.lstrip().startswith(":"):
            # Directive options like :widths: and :header-rows:
            continue

        row_start_match = re.match(r"^\s*\*\s*-\s*(.*)$", line)
        if row_start_match:
            if current_row:
                rows.append(current_row)
            current_row = [row_start_match.group(1).strip()]
            continue

        cell_match = re.match(r"^\s*-\s*(.*)$", line)
        if cell_match and current_row is not None:
            current_row.append(cell_match.group(1).strip())
            continue

        # Continuation line for the previous cell.
        if current_row:
            current_row[-1] = normalize_whitespace(
                f"{current_row[-1]} {line.strip()}".strip()
            )

    if current_row:
        rows.append(current_row)

    if not rows:
        return []

    candidate_headers = [normalize_header(cell) for cell in rows[0]]
    if looks_like_header_row(candidate_headers):
        headers = candidate_headers
        data_rows = rows[1:]
    else:
        headers = DEFAULT_HEADERS[: len(rows[0])]
        data_rows = rows

    return [cells_to_record(row, headers) for row in data_rows if any(row)]


def parse_list_tables(markdown_content: str) -> list[dict[str, str]]:
    records: list[dict[str, str]] = []
    for match in re.finditer(
        r"```{list-table}[^\n]*\n(?P<body>.*?)```", markdown_content, re.DOTALL
    ):
        block_records = parse_list_table_block(match.group("body"))
        records.extend(block_records)
    return records


def split_markdown_table_row(line: str) -> list[str]:
    stripped = line.strip().strip("|")
    return [cell.strip() for cell in stripped.split("|")]


def is_markdown_table_separator(line: str) -> bool:
    # Matches lines like | --- | :---: | ---: |
    return bool(re.match(r"^\s*\|?(?:\s*:?-{3,}:?\s*\|)+\s*:?-{3,}:?\s*\|?\s*$", line))


def parse_markdown_tables(markdown_content: str) -> list[dict[str, str]]:
    records: list[dict[str, str]] = []
    lines = markdown_content.splitlines()
    index = 0
    in_code_fence = False

    while index < len(lines):
        current = lines[index]
        if current.strip().startswith("```"):
            in_code_fence = not in_code_fence
            index += 1
            continue

        if in_code_fence or index + 1 >= len(lines):
            index += 1
            continue

        header_line = current
        separator_line = lines[index + 1]
        if "|" not in header_line or not is_markdown_table_separator(separator_line):
            index += 1
            continue

        raw_headers = split_markdown_table_row(header_line)
        normalized_headers = [normalize_header(cell) for cell in raw_headers]
        recognized = {header for header in normalized_headers if header in KNOWN_FIELDS}

        # Filter to probable contributor tables.
        if "name" not in recognized or len(recognized) < 3:
            index += 1
            continue

        index += 2
        while index < len(lines):
            row_line = lines[index]
            if not row_line.strip() or "|" not in row_line:
                break
            if row_line.strip().startswith("```"):
                break
            row_cells = split_markdown_table_row(row_line)
            if row_cells and any(cell.strip() for cell in row_cells):
                records.append(cells_to_record(row_cells, normalized_headers))
            index += 1
        continue

    return records


def deduplicate_records(records: list[dict[str, str]]) -> list[dict[str, str]]:
    seen: set[tuple[str, ...]] = set()
    unique_records: list[dict[str, str]] = []
    ordered_fields = ["role", "name", "github_handle", "institution", "date", "status"]
    for record in records:
        key = tuple(record.get(field, "") for field in ordered_fields)
        if key in seen:
            continue
        seen.add(key)
        unique_records.append(record)
    return unique_records


def split_people_field(value: str) -> list[str]:
    if not value:
        return []
    candidates = [part.strip() for part in re.split(r",|;", value)]
    return [candidate for candidate in candidates if candidate]


def normalize_role(role: str, status: str) -> str:
    role_text = strip_markdown(role).lower()
    status_text = strip_markdown(status).lower()
    combined = f"{role_text} {status_text}"

    if "author" in combined:
        return "author"
    if "reviewer" in combined or "review" in combined or "accept" in combined:
        return "reviewer"
    if "endorser" in combined or "endorse" in combined:
        return "endorser"
    if "commenter" in combined or "comment" in combined:
        return "commenter"
    if "editor" in combined:
        return "editor"
    if "implementer" in combined or "implemented" in combined:
        return "implementer"
    return "contributor"


def records_to_role_names(records: list[dict[str, str]]) -> dict[str, list[str]]:
    role_names: dict[str, list[str]] = defaultdict(list)
    for record in records:
        role = normalize_role(record.get("role", ""), record.get("status", ""))
        names = split_people_field(record.get("name", ""))
        if not names:
            continue
        for name in names:
            cleaned = strip_markdown(name)
            if not cleaned:
                continue
            if cleaned not in role_names[role]:
                role_names[role].append(cleaned)
    return dict(role_names)


def load_contribs_data() -> dict[str, Any]:
    if CONTRIBS_PATH.exists():
        return yaml.safe_load(CONTRIBS_PATH.read_text()) or {}
    return {}


if __name__ == "__main__":
    main()
